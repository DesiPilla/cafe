# Cafe: Predicting Physical Attractiveness with Deep Learning-Based Systems

## Abstract
The modern dating scene has been transformed by digital technologies. With the rise of social media apps such as Tinder and Bumble, finding a potential partner has never been so convenient. Can the process of finding a match be further digitized with deep learning techniques? This study explores the viability of using convolutional neural networks to create personalized matchmaking algorithms. Using transfer learning techniques, five architectures were appended to VGGNet, and trained on photos scraped from Google Images. Most networks were able to achieve an accuracy between 74% and 76%, competing with even the best wingman. This study also investigates how such models would perform on multiple images of a single person, which is intended to represent a dating profile. Most architectures exhibited reliable results, consistently giving the same label to four out of five images of a specific individual. Lastly, this paper explores the explainability of such models.

## Introduction
The way in which lovers are finding each other is, like all other domains, adapting to the wide-spread availability of technology. Popular dating apps such as Tinder, Bumble, and Plenty of Fish, among others, have amassed an estimated 30.4 million users in the United States and are expected to have 35.4 million users by 2024 [24]. The lure of these apps is that one can gain exposure to many potential partners quickly and conveniently. All a user must do is upload images of his- or herself, fill in some personal details, and explore. Most sites create a “match” if both users elect (anonymously) that they “like” the other user. This matching then enables the two users to communicate with each other and explore a potential relationship.

The efficacy of such apps for finding a long-term partner has been questioned by some; Bauman (2003) argues that the convenience of finding a partner undermines the principles of romance and courtship [3]. On mobile apps, the ease-of-use has made it easy to rapidly *swipe* (“like” or “dislike”) on others; Levy et al. (2019) found that male users spend 6.7 s evaluating a profile and female users spend 11.1 s [15]. Subtracted from this is the time required for face recognition, emotion awareness, and attraction preferences, which studies have found require 1 – 5 s for a person to process [28, 29, 30, 32]. This means online daters are spending only a few seconds making their decision. 

Researchers have found that physical attractiveness is often the most important contributor when forming an impression of a potential partner [19]. However, Kniffin and Wilson (2004) found that non-physical features that were learned only after interacting with others played a significant role in one’s attractiveness; a person’s initial assessment of someone’s physical attractiveness was different after obtaining further impressions through group activities. On dating apps, however, these personality traits are largely unavailable, save for self-reported characteristics in one’s bio. As evidenced by Levy et al. (2019), most swiping decisions are made in just enough time to process a user’s physical appearance.


### Motivation and problem statement
With the number of online singles growing, a person can get lost viewing profiles for many hours, never finding a single match. Waiting for your profile to be shown to others (and liked) can take time as well. On Tinder, a user can even be shown profiles they have already disliked as the pool of unviewed profiles dries up [27]. It has also been claimed that Tinder displays users that have already swiped left on you, in which case swiping on those profiles are meaningless. By increasing the volume and availability of prospective partners, these apps have also added an unintended inefficiency to this process. These undesirable app features, however, may be aimed more at maintaining a user base and following a business model than at efficient matchmaking.

To reduce wasted time, we propose using Deep Learning and transfer learning methods to automatically classify profiles for a given user. By training a personal algorithm, all matching would be done instantly (or in the required computational time to predict all profiles in the user-base) upon signing up for the service. Similar business models have been seen in social media apps such as Facebook, Instagram, and Twitter. Most recently, TikTok has exploited a prediction-based AI recommendation system for content feeds to foster substantial user-engagement rates. This is a large reason the average user spends 52 minutes per day on the app and the value of their parent company, ByteDance, grew to $75 million in November 2018 [7]. 

This study aims to provide a proof-of-concept for the idea and to define a model architecture that can be used generally to solve this task. Specifically, the model architecture was trained to predict physical attractiveness of a woman’s face according to the author’s personal preferences.


### Related works

Brozovsky and Petricek (2007) suggested collaborative filtering (CF) algorithms as an improvement on global algorithms [5]. These user-specific functions were better at producing a list of 10 profile recommendations for individuals that initially rated 150 profiles. Krzywicki et al. (2014) notes that standard profile matching algorithms are susceptible to “over-recommending” popular profiles [14]; this underscores the issue with a greedy approach used in certain online dating services. The study also found that including an additional rating system based on important user-specific features improved the success rate of real-life interactions.

Approaches using instance-based learning on user-specified features (eye color, hair color, age, hobbies, etc.) was found to have positive results as the user labeled increasing amounts of data [17]. Joel et al. (2017) used random forests on user-specified features to predict desire among individuals [10]. The results suggested that relationship science can meaningfully estimate people’s tendencies to desire others in general, but not to desire a single person in particular over others. That is, the distinction between predicting general attractiveness and unique allure requires different methodology.

Transfer learning techniques have been used to approach this classification problem. Vries and Yosinski (2015) compared the fine-tuned performance on two different pretrained models to classify attractiveness [31]. The first model was inspired by VGGNet [23] with fine-tuning layers added to the end. The second model was a similar convolutional neural network that was pre-trained on gender classification before fine-tuning. The VGGNet outperformed the gender classifier when classifying the attractiveness of 9,364 Tinder profile pictures. The fine-tuned VGGNet model obtained a 68.1% accuracy on the test set and performed subjectively well. The study also estimated the difficulty level of this task. After one week, the author who originally labeled the images according to his own personal preferences re-labeled a random sample of 100 images; this timeframe was chosen so the author would not remember or recognize any faces. Despite using his own preferences, he was only able to attain an 88% accuracy on the sample. The authors estimate from this that roughly 24% of images are not consistently labeled by humans, with approximately 12% defining a lower-bound on any error rates sought after for this task.

Though its underlying algorithm techniques are not publicly known, *iris* is a new dating app that claims to use artificial intelligence to learn what type of person a user finds visually attractive [9]. Dubbed “AttractionDNA”, the company implements a seemingly commercialized solution to the task this study will focus on.

To uncover the mystery behind how neural networks arrive at their outputs, Layer-wise Relevance Propagation was proposed as an improvement to sensitivity analysis [22]. This technique decomposes the learned function, assigning relevance to more active neurons and stronger edges. This propagates backwards to the input layer, at which point all input features (pixels, in this case) are given a *relevance score*. This score can be positive, meaning it helped support the final classification, or negative, meaning it contradicted the decision. The magnitude of these scores is proportional to the expected importance of the input values.

## Methods and data
The example task is to predict whether the author, a 20-year-old white male, would classify a photo of a woman to be pretty or not. The profiles this type of user would be exposed to include women between the ages of 18 and 25 (approximately) of all races, with perhaps a larger proportion of white women than other ethnicities.

## Google Images data
The first set of data used in this study was a collection of 1,040 representative images of young women that were scraped from Google. These images were used to train a model to predict preferences for potential partners. 

### Data acquisition
No adequately large collection of representative and labeled images could be found for our objective, so we constructed our own training set. 2,887 images were scraped from Google Images using defined search queries [21]. By entering the query term "young woman" into Google search, a fairly representative selection of images that a user would find on a dating app were returned. However, this yielded a disproportionately large number of white women, and very few images of minorities. To create a more diverse dataset (which is important for producing a robust and unbiased model), the search terms "young woman black", "young woman Hispanic", and "young woman Asian" were added.
Many of the scraped images contained a watermark that obstructed part or all of the face. This is problematic because a model may inadvertently "learn" the watermark as an indicative feature. In practical applications, the images fed into the model will not have watermarks. To avoid any issues, these images were not included in the final dataset. Other images were discarded for being irrelevant (animated pictures, logos, men) that were able to seep through the Google Search criteria. Roughly 59.6% of images were thrown out because there was a watermark overlayed on the face or they were irrelevant. This drastically reduced the number of images available, so the search term "young woman Instagram" was added. These photos were all very representative of what a profile picture might look like on a dating app.

After labeling these images, the resulting dataset contained a far larger number of *skip* (dislike) images than *sip* (like): 419 vs 276. To create an unbiased model, we wanted to use a balanced dataset. Therefore, the size of the dataset was limited to 276 observations of each class (before splitting into a training and validation set). This is not many observations. To artificially inflate the number of *sip* images available, the search term "young woman beautiful" was added. The new counts were 646 *skip* and 520 *sip* images. After balancing, the dataset is nearly double its previous size, a considerably larger set for training a model.

### Data augmentation / preprocessing
The images were displayed to the author without any augmentation or processing applied; the full, original image was classified as either sip or skip. Once labeled, the image was cropped to include only the face of the subject, identified using MTCNN [33] as implemented by Brownlee (2019) [4]. The cropped image is a different shape for each image, which is not appropriate for inputs to a neural network. As a workaround, the larger dimension was resized to 256 pixels, and the smaller dimension was scaled such that the aspect ratio was maintained. The smaller dimension was then padded with black pixels on both sides to a size of 256. The result was a 256x256 pixel image. A subset of the cropped images is displayed in Figure 1.

[ INSERT FIGURE 1 ]

When preparing training batches, the standard preprocessing for the VGG network was applied to all images [26]. This includes converting all images from RGB to BGR and zero-centering each color channel with respect to the ImageNet dataset (without scaling). Only one of the models (google1) did not apply this preprocessing when training.

To increase the number of training images available, transformations were also applied to the images when preparing training batches. The transformations included random rotation (up to 30 degrees), zoom (up to 15%), shift (up to 20% horizontally and vertically), and shear (up to 15%). This allows us to artificially inflate the size of our dataset when training.


### Data summary
The final dataset contains 1,040 images (520 of each class). Table 1 shows the composition of this dataset based on the query terms entered into Google Search.

[ INSERT TABLE ]

Notably, 23.07% of the total images are exclusively of minority women. This value is actually higher, as the search terms, “young woman”, “young woman beautiful”, and “young woman Instagram” may contain minorities as well. Generally, though, those groups were of mostly white women. 

The data was separated into a training and validation set by a random 80-20 split. 


### Data limitations
There is a significant gap in the proportion of minorities represented in the different classes. Minority women comprised 36.73% of all skip images, but only 9.42% of the *sip* images. This difference is nearly four-fold. It is important to note that this disparity may bias the inherent attractiveness of women of color, though any partiality will be inherently correlated with the preference of the author.

Furthermore, the training set was composed of images that would best represent the potential partners for a 20-year-old white male. Any results obtained using this dataset may not carry over to images containing men, older demographics, or more racially diverse groups.

Additionally, the images contain padded walls to ensure all images are the same shape. While this preserves the original proportions of the face, it introduces unrelated features that may be learned by a model. The paddings do not inherently add physical attractiveness to a person.
Lastly, a large bias was introduced by the way training images were labeled. To best simulate the environment of a dating app, the author was shown full, original images one-by-one and asked to classify each as either *sip* (like) or *skip* (dislike). The labels referred to the physical attractiveness of the entire image, with the assumption that an attractive person exhibits an attractive face. This premise is not entirely veristic, as there are other, non-facial features (body shape, clothing, pose, etc.) visible in the unaltered images that may have contributed to the evaluation of their attractiveness. While focusing on the face provides a convenient way to isolate the subject, it must be noted that the data has built-in inaccuracies to consider.


## Friends dataset

In addition to the 1,040 images scraped from Google Search, a second collection included a set of five images for each of 14 participants of the appropriate target group (18-25-year-old women). The images were not labeled by the author, but used as a subjective benchmark on real images, as well as to test for within-group variation in model predictions. In a practical environment, dating app-goers typically upload multiple images of themselves, but prospective matches can only give a single evaluation— “like” or “dislike”. While the user may find some photos more attractive than others, they ultimately must make a decision. This dataset can be used to determine how predictions vary for multiple images of the same person. All images were subject to the same data augmentation and preprocessing defined in Section 2.1.2.


## Methodology
Transfer learning is a very useful technique that allows us to train a deep learning model without requiring many training images. VGG16 is a very powerful and commonly used Convolutional Neural Network proposed by Simonyan and Zisserman [23]. The model architecture was trained on the ImageNet dataset (which contains over 14 million images of 1,000 objects) and achieved a top-5 accuracy of 92.7%. 

However, we do not need to classify 1,000 objects; we only want to distinguish between *sip* (like) and *skip* (dislike). We can utilize the VGG16 model by removing the final three (dense) layers. The convolutional layers that are kept are already initialized with trained weights to detect features within an image. This will save a lot of computational time (and reduce the required number of training images), as we do not need to start from scratch. The bottleneck features at the end of VGG16 can then be fed into a fresh set of dense layers in a structure we specify.

The training methodology follows popular techniques for transfer learning [2, 20]. There are two parts to model training: initialization and fine-tuning. In the initialization step, the weights of the pretrained VGG layers are frozen and not updated. The model is trained for a small number of epochs to generate *learned* values for the fresh layer weights as opposed to completely random values. In the fine-tuning step, the last set of the convolutional layers in the pretrained VGG network are unfrozen. This allows the model to home in on features that are more specific to our task. Fine-tuning is conducted over a longer period, with *early stopping* criteria defining how many epochs to use. During both steps, the optimizer used was SGD (*learning rate*=0.0001 and *momentum*=0.9). We chose to use a batch size of 32 for training, as smaller batch sizes have been found to boast better performances on the VGG16 network [11]. 

Five distinct models were trained and compared, each with different architectures (see Table 2) for their fresh layers. All models included
  * layer to flatten the final set of features from VGG
  * at least one fully connected layer (with between 128 and 1096 neurons) using “ReLu” as the activation function
  * dropout (with probability of 0.3 or 0.5)
  * a fully connected layer at the end with 2 outputs and a “softmax” activation function  
